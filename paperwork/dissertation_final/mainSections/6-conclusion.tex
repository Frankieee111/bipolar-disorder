
This work proposes an automatic framework for Bipolar Disorder (BD) recognition. As the BD corpus contains the clinical interviews with patients, all possible modalities in the interview recordings are utilized. 

For the audio-visual modality, Multimodal Deep Denoising Autoencoders (multi-DDAEs) are used to learn the joint representations, which are then encoded with the dynamics into Fisher Vectors (FVs). For the textual modality, document embeddings are inferred by Paragraph Vector (PV-DM) models that are based on the transcripts of interview videos. To reach the final decision, the early fusion strategy is applied to fuse features and for the classification task of mania levels, a Multi-Task Neural Network is implemented to address the overfitting issue with a joint loss on an addition regression task of YMRS score.

The multi-DDAEs take advantage of the shared representations from acoustic features, either MFCC or eGeMAPS, and visual features including facial landmarks, eye gaze, head pose and action units. On the other hand, the document embeddings produced by PV-DM provide complementary information on mental states and help to improve the classification performance in the final framework. The experimental results show that the proposed framework outperformed the baseline system and the frameworks \cite{du2018} and \cite{syed2018} in AVEC2018 with 0.709 in UAR and 0.717 in accuracy, and the cross-validation demonstrates its generalization across the BD corpus.

In the current framework, the multi-DDAEs focus the shared representation learning on all available modalities without considering the characteristics of each modality alone or the semantic interface between different modalities. Furthermore, the temporal information is simply captured by computing the dynamics of representations in contiguous frames. I plan to further the multi-DDAE architectures by introducing more layers (which could be Convolutional or Pooling layers) in the encoder part to capture the spatial information for some modalities, such as facial landmarks, and correlating and decoupling modalities via a sparse, semantics interface, like the convergence-divergence zone \cite{meyer2009}. Moreover, architectures like Recurrent Neural Networks (RNNs) are recommended to be investigated on encoding the shared representation within variable-length intervals. Finally, it would be worthwhile to experiment with the proposed framework on other but relevant mental state based challenges. AVEC2019 presents a challenge on human state-of-mind, which constantly shifts and is reflected by the emotions. I would like to apply this framework to this challenge and test its performance on other emotion-based recognition tasks.